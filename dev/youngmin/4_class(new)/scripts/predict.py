import os
import numpy as np
import torch
import torch.nn as nn
import librosa
import sounddevice as sd
from time import sleep

# ===== ì„¤ì • =====
SAMPLE_RATE = 22050
DURATION = 1  # 1ì´ˆ ë‹¨ìœ„ ì˜ˆì¸¡
N_MFCC = 13
MODEL_PATH = "model/cnn_audio_classifier.pth"
CLASS_NAMES = ["person", "cough", "laugh", "natural"]  # í´ë˜ìŠ¤ ì´ë¦„ ìˆœì„œ ì¤‘ìš”
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== ì…ë ¥ ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ =====
try:
    devices = sd.query_devices()
    input_devices = [i for i, d in enumerate(devices) if d['max_input_channels'] > 0]
    if not input_devices:
        raise RuntimeError("ì…ë ¥ ê°€ëŠ¥í•œ ë§ˆì´í¬ ë””ë°”ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    sd.default.device = input_devices[0]
    print(f"[INFO] ì…ë ¥ ë””ë°”ì´ìŠ¤ ì„¤ì •ë¨: {devices[sd.default.device]['name']}")
except Exception as e:
    print(f"[ERROR] ë§ˆì´í¬ ë””ë°”ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
    exit(1)

# ===== ëª¨ë¸ ì •ì˜ =====
class SimpleCNN(nn.Module):
    def __init__(self, input_shape, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d((2, 2)),
        )
        with torch.no_grad():
            dummy = torch.zeros(1, *input_shape)
            out = self.conv(dummy)
            self.flattened_dim = out.view(1, -1).shape[1]

        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.flattened_dim, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        return self.fc(x)

# ===== ëª¨ë¸ ë¡œë”© =====
print("[INFO] ëª¨ë¸ ë¡œë”© ì¤‘...")
model = SimpleCNN(input_shape=(1, 14, 87), num_classes=len(CLASS_NAMES)).to(DEVICE)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.eval()
print("[INFO] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# ===== ì˜ˆì¸¡ í•¨ìˆ˜ =====
def predict(audio):
    mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC)
    zcr = librosa.feature.zero_crossing_rate(audio)[0].reshape(1, -1)
    feature = np.concatenate([mfcc, zcr], axis=0)
    x = torch.tensor(feature, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1, 1, 14, T)
    with torch.no_grad():
        output = model(x)
        pred = output.argmax(1).item()
    return CLASS_NAMES[pred]

# ===== ì‹¤ì‹œê°„ ì˜ˆì¸¡ ë£¨í”„ =====
print("ğŸ™ ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ë‹¨)")

try:
    while True:
        print("âº ë…¹ìŒ ì¤‘...")
        audio = sd.rec(int(SAMPLE_RATE * DURATION), samplerate=SAMPLE_RATE, channels=1, dtype='float32')
        sd.wait()
        audio = audio.flatten()

        pred_class = predict(audio)
        print(f" ì˜ˆì¸¡ ê²°ê³¼: {pred_class}")
        sleep(0.5)

except KeyboardInterrupt:
    print("\n[INFO] ì˜ˆì¸¡ ì¢…ë£Œë¨.")

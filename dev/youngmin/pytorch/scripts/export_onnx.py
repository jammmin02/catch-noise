import torch
import torch.nn as nn
import numpy as np

# ğŸ”¸ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (train.pyì™€ ë™ì¼í•´ì•¼ í•¨)
class AudioCNN(nn.Module):
    def __init__(self):
        super(AudioCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.pool1 = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(4032, 64)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(64, 1)

    def forward(self, x):
        x = torch.relu(self.pool1(self.conv1(x)))
        x = torch.relu(self.pool2(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc2(x))
        return x

# ğŸ”¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = AudioCNN()
model.load_state_dict(torch.load("../model/cnn_model.pt", map_location="cpu"))
model.eval()

# ğŸ”¸ ë”ë¯¸ ì…ë ¥ (ì‹¤ì œ ì…ë ¥ê³¼ ê°™ì€ shape)
dummy_input = torch.randn(1, 1, 86, 14)  # (Batch=1, Channels=1, Height=86, Width=14)

# ğŸ”¸ ONNXë¡œ ë‚´ë³´ë‚´ê¸°
torch.onnx.export(
    model,
    dummy_input,
    "../model/cnn_model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
    opset_version=11
)

print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ model/cnn_model.onnx íŒŒì¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

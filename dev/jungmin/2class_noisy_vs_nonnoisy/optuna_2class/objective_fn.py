import torch
import torch.nn as nn
from torch.optim import Adam
from sklearn.metrics import accuracy_score
import mlflow
import time
from model import CNNLSTM

# âœ… Trialë³„ ë…ë¦½ì ì¸ MLflow Run ìƒì„± + ìµœì†Œ ê¸°ë¡
def safe_log_mlflow(trial, acc, max_retries=3, wait_sec=1):
    acc = round(acc, 4)
    for attempt in range(max_retries):
        try:
            with mlflow.start_run(run_name=f"trial_{trial.number}"):  # âœ… ê°œë³„ run
                for k, v in trial.params.items():
                    mlflow.log_param(k, v)
                mlflow.log_metric("val_accuracy", acc)
            time.sleep(wait_sec)
            return True
        except Exception as e:
            print(f"[âš ï¸ MLflow ê¸°ë¡ ì‹¤íŒ¨ - Trial {trial.number}, ì‹œë„ {attempt+1}] {e}")
            time.sleep(2)
    print(f"âŒ Trial {trial.number} val_accuracy ê¸°ë¡ ì‹¤íŒ¨")
    return False

# âœ… Optunaìš© objective í•¨ìˆ˜ (ë°”ë€ êµ¬ì¡° ë°˜ì˜)
def objective(trial, device, X_train, X_val):
    # ðŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    conv1 = trial.suggest_categorical("conv1_filters", [16, 32, 64])
    conv2 = trial.suggest_categorical("conv2_filters", [32, 64, 128])
    lstm_units = trial.suggest_int("lstm_units", 32, 128, step=32)
    dense_units = trial.suggest_int("dense_units", 32, 128, step=32)
    dropout = trial.suggest_float("dropout", 0.2, 0.5, step=0.1)
    lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])

    # âœ… ë°”ë€ êµ¬ì¡°ì— ë§žê²Œ CNNLSTM ìƒì„± (timesteps, features ì œê±°ë¨)
    model = CNNLSTM(
        conv1_filters=conv1,
        conv2_filters=conv2,
        lstm_units=lstm_units,
        dense_units=dense_units,
        dropout=dropout
    ).to(device)

    loss_fn = nn.BCELoss()
    optimizer = Adam(model.parameters(), lr=lr)

    # âœ… í•™ìŠµ
    for epoch in range(10):
        model.train()
        for xb, yb in X_train:
            xb, yb = xb.to(device), yb.to(device)
            preds = model(xb)
            loss = loss_fn(preds, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # âœ… í‰ê°€
    model.eval()
    preds, targets = [], []
    with torch.no_grad():
        for xb, yb in X_val:
            xb = xb.to(device)
            output = model(xb)
            pred_np = (output.cpu().detach().numpy() > 0.5).astype(int).tolist()
            target_np = yb.numpy().astype(int).tolist()
            preds.extend(pred_np)
            targets.extend(target_np)

    acc = accuracy_score(targets, preds)
    safe_log_mlflow(trial, acc)
    return 1.0 - acc  # minimize

import torch
import torch.nn as nn
from torch.optim import Adam
from sklearn.metrics import accuracy_score
import mlflow
import time
from model import CNNLSTM
from data_loader import load_data

# âœ… MLflow ì•ˆì „ ë¡œê·¸ í•¨ìˆ˜
def safe_log_mlflow(trial, acc, max_retries=3, wait_sec=1):
    acc = round(acc, 4)
    for attempt in range(max_retries):
        try:
            with mlflow.start_run(run_name=f"trial_{trial.number}"):
                for k, v in trial.params.items():
                    mlflow.log_param(k, v)
                mlflow.log_metric("val_accuracy", acc)
            time.sleep(wait_sec)
            return True
        except Exception as e:
            print(f"[âš ï¸ MLflow ê¸°ë¡ ì‹¤íŒ¨ - Trial {trial.number}, ì‹œë„ {attempt+1}] {e}")
            time.sleep(2)
    print(f"âŒ Trial {trial.number} val_accuracy ê¸°ë¡ ì‹¤íŒ¨")
    return False

# âœ… Optuna objective í•¨ìˆ˜
def objective(trial, device):
    try:
        # ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
        conv1 = trial.suggest_categorical("conv1_filters", [16, 32, 64])
        conv2 = trial.suggest_categorical("conv2_filters", [32, 64, 128])
        lstm_units = trial.suggest_int("lstm_units", 32, 128, step=32)
        dense_units = trial.suggest_int("dense_units", 32, 128, step=32)
        dropout = trial.suggest_float("dropout", 0.2, 0.5, step=0.1)
        lr = trial.suggest_float("lr", 1e-4, 1e-2, log=True)
        batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])

        print(f"\nğŸš€ [Trial {trial.number}] ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„°: conv1={conv1}, conv2={conv2}, lstm={lstm_units}, dense={dense_units}, dropout={dropout}, lr={lr:.5f}, batch_size={batch_size}")

        # âœ… ë°ì´í„° ë¡œë”©
        train_loader, val_loader, _ = load_data(batch_size)

        # âœ… ëª¨ë¸ ì •ì˜
        model = CNNLSTM(conv1, conv2, lstm_units, dense_units, dropout).to(device)
        loss_fn = nn.BCELoss()
        optimizer = Adam(model.parameters(), lr=lr)

        # âœ… í•™ìŠµ
        for epoch in range(1, 11):
            model.train()
            running_loss = 0.0
            for xb, yb in train_loader:
                xb, yb = xb.to(device), yb.to(device)
                if xb.dim() == 3:  # ì±„ë„ ì°¨ì› ì—†ìœ¼ë©´ ì¶”ê°€
                    xb = xb.unsqueeze(1)
                yb = yb.view(-1, 1)

                preds = model(xb)
                loss = loss_fn(preds, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            avg_loss = running_loss / len(train_loader)
            print(f"ğŸ“˜ Epoch {epoch:2d}/10 - í‰ê·  Loss: {avg_loss:.4f}")

        # âœ… ê²€ì¦
        model.eval()
        preds, targets = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                if xb.dim() == 3:
                    xb = xb.unsqueeze(1)
                yb = yb.view(-1, 1)

                output = model(xb).cpu().squeeze().numpy()
                target_np = yb.cpu().squeeze().numpy()
                pred_np = (output > 0.5).astype(int).tolist()
                target_np = target_np.astype(int).tolist()

                preds.extend(pred_np)
                targets.extend(target_np)

        acc = accuracy_score(targets, preds)
        print(f"âœ… [Trial {trial.number}] ì™„ë£Œ - val_accuracy: {acc:.4f}")

        # âœ… MLflow ë¡œê·¸
        safe_log_mlflow(trial, acc)
        return 1.0 - acc

    except Exception as e:
        print(f"âŒ Trial {trial.number} ì‹¤íŒ¨: {str(e)}")
        return float("inf")

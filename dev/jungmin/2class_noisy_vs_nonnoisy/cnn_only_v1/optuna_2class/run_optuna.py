import os
import json
import optuna
import mlflow
import torch
from datetime import datetime
from model import CNNOnly
from objective_fn import objective
from data_loader import load_data
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

torch.backends.cudnn.enabled = False

# âœ… MLflow ì„¤ì •
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://210.101.236.174:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# âœ… ì‹¤í—˜ ì´ë¦„
now = datetime.now()
EXPERIMENT_NAME = f"optuna_cnn_2class_{now.strftime('%Y%m%d_%H%M%S')}"
mlflow.set_experiment(EXPERIMENT_NAME)

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    N_TRIALS = 20
    trial_counter = {"current": 0}

    # âœ… Optuna objective í•¨ìˆ˜ ë˜í•‘
    def obj(trial):
        trial_counter["current"] += 1
        print(f"\nğŸ§ª Trial {trial_counter['current']}/{N_TRIALS} ì‹œì‘ ì¤‘... (Optuna Trial #{trial.number})")
        try:
            return objective(trial, device)
        except Exception as e:
            print(f"âš ï¸ Trial {trial.number} ì‹¤íŒ¨: {str(e)}")
            return float("inf")

    # âœ… Optuna ì‹¤í–‰
    print(f"ğŸ“Š ì´ {N_TRIALS}ê°œì˜ Trialì„ ì‹¤í–‰í•©ë‹ˆë‹¤...\n")
    study = optuna.create_study(direction="minimize")
    study.optimize(obj, n_trials=N_TRIALS)

    # âœ… Best Trial ê²°ê³¼
    best = study.best_trial
    best_val_acc = 1.0 - best.value
    print(f"\nğŸ‰ âœ… Best Trial {best.number} ì™„ë£Œ!")
    print(f"ğŸ† Validation Accuracy: {best_val_acc:.4f}")
    print(f"ğŸ“Œ Best Params:\n{json.dumps(best.params, indent=2)}")

    # âœ… MLflow ê¸°ë¡
    with mlflow.start_run(run_name=f"best_trial_{best.number}"):
        for k, v in best.params.items():
            mlflow.log_param(k, v)
        mlflow.log_metric("best_val_accuracy", best_val_acc)

        os.makedirs("outputs/cnn_only", exist_ok=True)
        with open("outputs/cnn_only/best_params.json", "w") as f:
            json.dump(best.params, f)
        mlflow.log_artifact("outputs/cnn_only/best_params.json")

        # âœ… ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ì¤€ë¹„
        model = CNNOnly(
            conv1_filters=best.params["conv1_filters"],
            conv2_filters=best.params["conv2_filters"],
            dense_units=best.params["dense_units"],
            dropout=best.params["dropout"]
        ).to(device)

        train_loader, val_loader, _ = load_data(best.params["batch_size"])
        loss_fn = torch.nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=best.params["lr"])

        # âœ… ì¬í•™ìŠµ
        print("\nğŸ“¦ Best ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì¬í•™ìŠµ)...")
        for epoch in range(1, 11):
            model.train()
            running_loss = 0.0
            for xb, yb in train_loader:
                xb, yb = xb.to(device), yb.to(device)
                yb = yb.view(-1, 1)

                preds = model(xb)
                loss = loss_fn(preds, yb)
                optimizer.zero_grad()
                optimizer.step()
                running_loss += loss.item()
            avg_loss = running_loss / len(train_loader)
            print(f"ğŸ“˜ Epoch {epoch:2d}/10 - í‰ê·  Loss: {avg_loss:.4f}")

        # âœ… ëª¨ë¸ ì €ì¥
        model_path = "outputs/cnn_only/best_model.pt"
        torch.save(model.state_dict(), model_path)
        mlflow.log_artifact(model_path)
        print(f"\nâœ… Best model ì €ì¥ ë° ë¡œê·¸ ì™„ë£Œ â†’ {model_path}")

        # âœ… ê²€ì¦ í‰ê°€ ë° ì‹œê°í™”
        model.eval()
        preds, targets = [], []
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                yb = yb.view(-1, 1)

                output = model(xb).cpu().squeeze().numpy()
                target_np = yb.cpu().squeeze().numpy()
                pred_np = (output > 0.5).astype(int).tolist()
                target_np = target_np.astype(int).tolist()

                preds.extend(pred_np)
                targets.extend(target_np)

        acc = accuracy_score(targets, preds)
        f1 = f1_score(targets, preds)
        cm = confusion_matrix(targets, preds)

        mlflow.log_metric("final_val_accuracy", acc)
        mlflow.log_metric("final_val_f1_score", f1)

        # ğŸ¨ í˜¼ë™í–‰ë ¬ ì‹œê°í™”
        plt.figure(figsize=(5, 4))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
        plt.title("Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.tight_layout()
        plt.savefig("confusion_matrix.png")
        mlflow.log_artifact("confusion_matrix.png")

        # ğŸ¨ F1 Score ê·¸ë˜í”„
        plt.figure()
        plt.bar(["F1 Score"], [f1])
        plt.ylim(0, 1)
        plt.title("F1 Score")
        plt.savefig("f1_score.png")
        mlflow.log_artifact("f1_score.png")

        # ğŸ“ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥
        with open("best_report.txt", "w") as f:
            f.write(f"ğŸ“Œ Best Trial #{best.number}\n")
            f.write(f"Validation Accuracy: {acc:.4f}\n")
            f.write(f"F1 Score: {f1:.4f}\n\n")
            f.write("Classification Report:\n")
            f.write(classification_report(targets, preds, digits=4))
        mlflow.log_artifact("best_report.txt")

if __name__ == "__main__":
    main()

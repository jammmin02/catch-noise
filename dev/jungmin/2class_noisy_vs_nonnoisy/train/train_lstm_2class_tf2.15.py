import os
import numpy as np
import matplotlib.pyplot as plt
import mlflow
import mlflow.tensorflow
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Reshape, LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.losses import BinaryCrossentropy

# ‚úÖ ÎÇ†Ïßú Í∏∞Î∞ò Ïã§Ìóò Ïù¥Î¶Ñ
today = datetime.now().strftime("%Y%m%d")
experiment_name = f"train_cnn_lstm_2class_jungmin_{today}"

# ‚úÖ MLflow ÏÑ§Ï†ï
mlflow.set_tracking_uri("file:/app/mlruns")
mlflow.set_experiment(experiment_name)

with mlflow.start_run():
    # üîß Í≤ΩÎ°ú ÏÑ§Ï†ï
    segment_duration = 3.0
    base_dir = "dev/jungmin/2class_noisy_vs_nonnoisy/outputs/cnn_lstm"
    X_path = os.path.join(base_dir, "X_lstm.npy")
    y_path = os.path.join(base_dir, "y_lstm.npy")
    model_save_path = os.path.join(base_dir, "cnn_lstm_model.h5")
    plot_save_path = os.path.join(base_dir, f"train_history_segment{int(segment_duration)}s.png")
    confusion_path = os.path.join(base_dir, "confusion_matrix_segment3s.png")
    confidence_plot_path = os.path.join(base_dir, "sigmoid_confidence_hist.png")
    model_summary_path = os.path.join(base_dir, "model_summary.txt")
    label_names = ['non_noisy', 'noisy']

    # üì• Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X = np.load(X_path)
    y = np.load(y_path)
    X = X[..., np.newaxis]  # CNNÏö© Ï±ÑÎÑê Ï∞®Ïõê Ï∂îÍ∞Ä

    # üìä Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=2/9, stratify=y_temp, random_state=42)

    # üß† Î™®Îç∏ Íµ¨ÏÑ±
    timesteps = X.shape[1] // 4
    features = (X.shape[2] // 4) * 64

    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(X.shape[1], X.shape[2], 1)),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        MaxPooling2D((2, 2)),
        Reshape((timesteps, features)),
        LSTM(64),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss=BinaryCrossentropy(), metrics=['accuracy'])

    # üìÑ Î™®Îç∏ Íµ¨Ï°∞ Ï†ÄÏû•
    with open(model_summary_path, "w") as f:
        model.summary(print_fn=lambda x: f.write(x + "\n"))
    mlflow.log_artifact(model_summary_path)

    # üßæ ÌååÎùºÎØ∏ÌÑ∞ Í∏∞Î°ù
    mlflow.log_params({
        "segment_duration": segment_duration,
        "optimizer": "adam",
        "loss": "binary_crossentropy",
        "batch_size": 32,
        "epochs": 30,
        "architecture": "cnn_lstm"
    })

    # üîÅ ÌïôÏäµ ÏãúÏûë
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=30,
        batch_size=32,
        callbacks=[early_stop],
        verbose=1
    )

    # üß™ ÌÖåÏä§Ìä∏ ÌèâÍ∞Ä
    test_loss, test_acc = model.evaluate(X_test, y_test)
    mlflow.log_metrics({
        "test_accuracy": test_acc,
        "test_loss": test_loss
    })

    # üíæ Î™®Îç∏ Ï†ÄÏû• Î∞è Î°úÍ∑∏
    model.save(model_save_path)
    mlflow.log_artifact(model_save_path)

    # üìà ÌïôÏäµ Í≤∞Í≥º Í∑∏ÎûòÌîÑ
    def smooth_curve(points, factor=0.6):
        smoothed = []
        for point in points:
            if smoothed:
                smoothed.append(smoothed[-1] * factor + point * (1 - factor))
            else:
                smoothed.append(point)
        return smoothed

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Acc', marker='o')
    plt.plot(history.history['val_accuracy'], label='Val Acc', marker='x')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(smooth_curve(history.history['loss']), label='Train Loss (Smoothed)', marker='o')
    plt.plot(smooth_curve(history.history['val_loss']), label='Val Loss (Smoothed)', marker='x')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(plot_save_path)
    plt.show()
    mlflow.log_artifact(plot_save_path)

    # üìä ÌòºÎèô ÌñâÎ†¨ ÏãúÍ∞ÅÌôî
    y_pred = model.predict(X_test)
    y_pred_classes = (y_pred > 0.5).astype(int).reshape(-1)
    cm = confusion_matrix(y_test, y_pred_classes)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)
    disp.plot(cmap='Blues')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig(confusion_path)
    plt.show()
    mlflow.log_artifact(confusion_path)

    # üîç Confidence histogram
    confidences = y_pred.flatten()
    plt.figure(figsize=(8, 5))
    plt.hist(confidences, bins=20, color='skyblue', edgecolor='black')
    plt.title("Sigmoid Confidence Distribution (Test Set)")
    plt.xlabel("Confidence")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(confidence_plot_path)
    plt.show()
    mlflow.log_artifact(confidence_plot_path)

print(f"‚úÖ MLflow experiment '{experiment_name}' Ï†ÄÏû• ÏôÑÎ£å!")
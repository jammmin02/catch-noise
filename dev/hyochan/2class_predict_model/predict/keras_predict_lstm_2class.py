import sounddevice as sd
import librosa
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
import threading
import time

# ğŸ› ì„¤ì •
sr = 22050
segment_duration = 2.0  # ì˜ˆì¸¡ìš© segment ê¸¸ì´ (ì´ˆ)
n_mfcc = 13
hop_length = 512
frame_per_second = sr / hop_length
max_len = int(frame_per_second * segment_duration)
class_names = ['non_noisy', 'noisy']
model_path = "C:/Users/USER/.aCode/catch-noise/dev/hyochan/2class_predict_model/dataset/outputs/cnn_lstm/cnn_lstm_model.keras"
model = load_model(model_path)

# ğŸ”Š ì˜¤ë””ì˜¤ ë²„í¼
rolling_audio = np.zeros(int(segment_duration * sr), dtype=np.float32)

# ğŸ§  ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜ (ë©”ì¸ ë£¨í”„ì—ì„œ ì ‘ê·¼)
latest_pred = [0.5, 0.5]
latest_label = "non_noisy"
latest_confidence = 0.5

# ğŸ¨ ì‹œê°í™” ì´ˆê¸°í™”
plt.ion()
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))

# (1) ì˜ˆì¸¡ ê²°ê³¼ ë§‰ëŒ€ ê·¸ë˜í”„ (ìƒë‹¨)
bar_plot = ax1.bar(class_names, latest_pred, color=["skyblue", "salmon"])
ax1.set_ylim(0, 1)
ax1.set_title("Prediction results")
ax1.set_ylabel("Probability")

# (2) ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ íŒŒí˜• (í•˜ë‹¨)
plot_len = int(sr * 0.5)
line_wave, = ax2.plot(np.zeros(plot_len))
ax2.set_ylim(-1, 1)
ax2.set_title("Enter real-time microphone")
ax2.set_xlabel("Sample")
ax2.set_ylabel("Amplitude")

# ğŸ› íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_features(y_audio):
    mfcc = librosa.feature.mfcc(y=y_audio, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)
    zcr = librosa.feature.zero_crossing_rate(y=y_audio, hop_length=hop_length)
    features = np.vstack([mfcc, zcr])
    if features.shape[1] < max_len:
        features = np.pad(features, ((0, 0), (0, max_len - features.shape[1])), mode='constant')
    else:
        features = features[:, :max_len]
    return features.T[np.newaxis, ..., np.newaxis]  # (1, max_len, 14, 1)

# ğŸ¤ ë§ˆì´í¬ ì½œë°± (0.05ì´ˆ ë‹¨ìœ„)
def audio_callback(indata, frames, time_info, status):
    global rolling_audio
    indata = indata[:, 0]
    rolling_audio = np.roll(rolling_audio, -len(indata))
    rolling_audio[-len(indata):] = indata

# ğŸ§  ì˜ˆì¸¡ ì“°ë ˆë“œ (1ì´ˆë§ˆë‹¤ ì¤‘ì²©ëœ 2ì´ˆ êµ¬ê°„ ì˜ˆì¸¡)
def predict_thread():
    global rolling_audio, latest_pred, latest_label, latest_confidence
    segment_len = int(sr * segment_duration)  # 2ì´ˆ segment

    while True:
        time.sleep(1.0)  # 1ì´ˆë§ˆë‹¤ ì˜ˆì¸¡ (50% ì¤‘ì²©)
        if len(rolling_audio) >= segment_len:
            audio_seg = rolling_audio[-segment_len:]
            x = extract_features(audio_seg)
            pred = model.predict(x, verbose=0)[0]
            label_idx = np.argmax(pred)
            latest_pred = pred
            latest_label = class_names[label_idx]
            latest_confidence = pred[label_idx]
            print(f"\nğŸ” ì¤‘ì²© ì˜ˆì¸¡: [{latest_label}] (ì‹ ë¢°ë„: {latest_confidence:.2f})")

# ğŸ¬ ì‹¤í–‰
print("ğŸ™ï¸ ì‹¤ì‹œê°„ íŒŒí˜• + ì¤‘ì²© ì˜ˆì¸¡ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)...")
stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=sr, blocksize=int(sr * 0.05))
stream.start()

# ğŸ§µ ì˜ˆì¸¡ ì“°ë ˆë“œ ì‹œì‘
threading.Thread(target=predict_thread, daemon=True).start()

# ğŸ“ˆ ë©”ì¸ ë£¨í”„: ì‹¤ì‹œê°„ íŒŒí˜• + ì˜ˆì¸¡ ê·¸ë˜í”„ ì‹œê°í™”
try:
    while True:
        # ì‹¤ì‹œê°„ íŒŒí˜• ì—…ë°ì´íŠ¸
        line_wave.set_ydata(rolling_audio[-plot_len:])

        # ì˜ˆì¸¡ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        for i, bar in enumerate(bar_plot):
            bar.set_height(latest_pred[i])
        ax1.set_title(f"ğŸ”® ì˜ˆì¸¡ ê²°ê³¼: [{latest_label}] (ì‹ ë¢°ë„: {latest_confidence:.2f})")

        # ì „ì²´ í”Œë¡¯ ê°±ì‹ 
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(0.05)

except KeyboardInterrupt:
    print("\nğŸ›‘ ì¢…ë£Œë¨.")
    stream.stop()
    plt.ioff()
    plt.close()
